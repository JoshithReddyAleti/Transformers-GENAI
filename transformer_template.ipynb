{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7d2897",
   "metadata": {},
   "source": [
    "# Transformer Text Generation\n",
    "\n",
    "In this notebook, we will explore how transformer models (like GPT-2) can generate text based on a given prompt. We will experiment with generating text by adjusting parameters like temperature and sequence length.\n",
    "\n",
    "## Instructions\n",
    "1. Change the prompt below to experiment with different types of text generation.\n",
    "2. Adjust the `max_length` and `temperature` parameters to see how they affect the output.\n",
    "3. Generate at least 3 samples with different prompts and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbce095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joshi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the future, education will be critical in the fight against climate change, as we are facing a rapidly changing climate and must have a plan in place to help meet that challenge,\" said Dr. Richard G. Kieren, director of the Center\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 text generation model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Set your prompt\n",
    "prompt = 'In the future, education will'\n",
    "\n",
    "# Generate text\n",
    "result = generator(prompt, max_length=50, temperature=0.7)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69a033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The impact of AI on the future of work will depend on how it will be implemented. While it is certainly true that we will see a variety of AI innovations from companies like Google to Facebook and Amazon to Microsoft, which will allow for better collaborative work\n",
      "Once upon a time, there was a kingdom of saints. This kingdom was called the kingdom of Christ. The name of this kingdom was the kingdom of the saints. This kingdom was called the kingdom of the saints.\n",
      "\n",
      "The kingdom of Christ was called the kingdom of the saints. This kingdom was called the kingdom of the saints. The kingdom of Christ was called the kingdom of the saints.\n",
      "\n",
      "The kingdom of Christ was called the kingdom of the saints. This kingdom was called the kingdom of\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different prompts\n",
    "prompt = 'The impact of AI on the future of work'\n",
    "result = generator(prompt, max_length=50, temperature=0.8)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "prompt = 'Once upon a time, there was a kingdom'\n",
    "result = generator(prompt, max_length=100, temperature=0.6)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c56a7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- News Headline Outputs ---\n",
      "Output 1: Breaking news: Scientists discover a new'superior' gene\n",
      "\n",
      "Researchers at Harvard Medical School have discovered that the transcriptional regulation of DNA is highly conserved, allowing for improved survival in patients with advanced diseases such as heart disease or cancer.\n",
      "\n",
      "\n",
      "The scientists, led by Harvard Medical School research\n",
      "Output 2: Breaking news: Scientists discover a new way to extract methane from fossil fuels\n",
      "\n",
      "A team led by Australian research scientist Greg McBride, also from the University of Adelaide, has discovered a method that can extract the methane from ice-age petroleum resources â€“ a process known as ice-age hydrolysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Short Story Output ---\n",
      "It was a cold and stormy night when I woke up in the middle of the night. I was lying on the floor in my room, trying to keep my head up. I noticed a hole in the ceiling. I turned around and saw the girl lying on the floor. I looked down and saw her face. She was naked. She had a hole in her face. I could see it through her hair. I told her to look at me. She said, \"I need to see\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dialogue Output ---\n",
      "Alice: Where are we going?\n",
      "Bob: I just got off the bus.\n",
      "Bob: Oh, I'm glad to see you.\n",
      "Bob: I'm sorry.\n",
      "Bob: Why are you asking about me?\n",
      "Bob: Well\n",
      "\n",
      "--- Factual Question Outputs ---\n",
      "Output 1: What are the benefits of renewable energy?\n",
      "\n",
      "A. Renewable energy has the potential to significantly reduce greenhouse gas emissions. It also has the potential to reduce the amount of energy that is wasted in the economy.\n",
      "\n",
      "Q. What is the cost of renewable energy?\n",
      "\n",
      "A. The cost of renewable energy is the cost of electricity.\n",
      "Output 2: What are the benefits of renewable energy?\n",
      "\n",
      "The benefits of renewable energy are obvious. The cost of electricity is lower than in the past. The cost of energy is lower than in the past. The cost of energy is lower than in the past. The cost of energy is lower than in the past. The cost of energy is lower than in\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# 1. News Headline Prompt\n",
    "prompt_1 = \"Breaking news: Scientists discover a new\"\n",
    "result_1 = generator(prompt_1, max_length=60, temperature=0.9, num_return_sequences=2)\n",
    "print(\"\\n--- News Headline Outputs ---\")\n",
    "for i, output in enumerate(result_1):\n",
    "    print(f\"Output {i+1}:\", output['generated_text'])\n",
    "\n",
    "# 2. Short Story Opener\n",
    "prompt_2 = \"It was a cold and stormy night when\"\n",
    "result_2 = generator(prompt_2, max_length=100, temperature=0.6, num_return_sequences=1)\n",
    "print(\"\\n--- Short Story Output ---\")\n",
    "print(result_2[0]['generated_text'])\n",
    "\n",
    "# 3. Dialogue\n",
    "prompt_3 = \"Alice: Where are we going?\\nBob:\"\n",
    "result_3 = generator(prompt_3, max_length=50, temperature=0.7, num_return_sequences=1)\n",
    "print(\"\\n--- Dialogue Output ---\")\n",
    "print(result_3[0]['generated_text'])\n",
    "\n",
    "# 4. Question or Factual Statement\n",
    "prompt_4 = \"What are the benefits of renewable energy?\"\n",
    "result_4 = generator(prompt_4, max_length=70, temperature=0.5, num_return_sequences=2)\n",
    "print(\"\\n--- Factual Question Outputs ---\")\n",
    "for i, output in enumerate(result_4):\n",
    "    print(f\"Output {i+1}:\", output['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d0d32",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Now that you have experimented with text generation, write a brief report on your observations.\n",
    "\n",
    "1. What patterns did you notice in the generated text?\n",
    "2. How did changing the temperature affect the creativity and coherence of the text?\n",
    "3. What types of prompts yielded the most coherent results?\n",
    "4. What are the limitations of GPT-2 based on your experimentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11142684",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
